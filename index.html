
<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<link rel="stylesheet" href="css/bootstrap.min.css">
		<link rel="stylesheet" type="text/css" href="css/style.css">
		<link href='http://fonts.googleapis.com/css?family=Lato:100,300,400' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Roboto:400,300,100' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400' rel='stylesheet' type='text/css'>

		<script type="text/javascript" src="js/bootstrap.min.js"></script>

		<title>Fuwen's Homepage</title>
	</head>
	<body>
		<div class="container">
			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h1 class="title-super">Fuwen TAN</h1>
				</div>
			</div> <!-- class="row" -->
			<div class="row row-thin">
				<div class="col-md-6 text-left">
					<h4 class="header-thin">Graduate Student</h4>
					<h4 class="header-thin">Department of Computer Science</h4>
					<h4 class="header-thin">University of Virginia</h4>
				</div>
				<div class="col-md-6 text-right flex-col">
					<h4 class="header-thin">85 Engineer's Way, Box 400740</h4>
					<h4 class="header-thin">Charlottesville, VA 22904</h4>
					<h4 class="header-thin">fuwen.tan@virginia.edu</h4>
				</div>
			</div> <!-- class="row" -->
			<div class="row row-thin">
				<div class="col-md-12">
					<hr>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>About me</h2>
				</div>
				<div class="col-md-12 text-thin">
					<p>I am a Ph.D. student in the <a href="http://www.cs.virginia.edu/">Computer Science Department</a> of U.Va, working with <a href="http://vicenteordonez.com/">Dr. Vicente Ord&oacute;&ntilde;ez Rom&aacute;n</a>
						on Vision and Language. 
						I am especially interested in learning compositional representations of image and language, and their applications to visual recognition, retrieval, and synthesis.</p>


					<p>Before joining U.Va., I was a Research Associate at the BeingThere center of Nanyang Technological University, working on Computer Graphics and 3D Telepresence.
					Here is my <a href="docs/fwtan-cv.pdf">CV</a>.</p>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Publications</h2>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/text2scene.png">
				</div>
				<div class="col-md-9">
					<h4>Text2Scene: Generating Compositional Scenes from Textual Descriptions</h4>
					<h5><strong>Fuwen Tan</strong>, Song Feng, Vicente Ordonez</h5>
					<h5>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019, <strong>Oral presentation</strong></h5>
					<p>We propose Text2Scene, a model that interprets input natural language descriptions in order to generate various forms of compositional scene representations; from abstract cartoon-like scenes to synthetic images. Unlike recent works, our method does not use generative adversarial networks, but a combination of an encoder-decoder model with a semi-parametric retrieval-based approach. 
					<a href="https://arxiv.org/abs/1809.01110">...</a>
					</p>
					<h5>[ <a href="https://arxiv.org/abs/1809.01110">Arxiv paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/Text2Image">code (coming soon)</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_text2scene.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/who_where.jpg">
				</div>
				<div class="col-md-9">
					<h4>Where and Who? Automatic Semantic-Aware Person Composition</h4>
					<h5><strong>Fuwen Tan</strong>, Crispin Bernier, Benjamin Cohen, Vicente Ordonez, Connelly Barnes</h5>
					<h5>IEEE Winter Conf. on Applications of Computer Vision (WACV), 2018</h5>
					<p>Image compositing is a popular and successful method used to generate realistic yet fake imagery. Much previous work in compositing has focused on improving the appearance compatibility between a given object segment and a background image. However, most previous work does not investigate the topic of automatically selecting semantically compatible segments and predicting their locations and sizes given a background image.
					<a href="https://arxiv.org/abs/1706.01021">...</a>
					</p>
					<!-- <h5>[ <a href="https://arxiv.org/abs/1706.01021">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_who_where.bib">bibtex</a> ]</h5> -->
					<h5>[ <a href="docs/composites_preprint.pdf">paper</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="docs/composites_supp.pdf">supplemental PDF</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="https://github.com/fwtan/who_where">code</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="docs/composites_demo.mp4">video</a> ] &nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_who_where.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/facecollage.png">
				</div>
				<div class="col-md-9">
					<h4>FaceCollage: A Rapidly Deployable System for Real-time Head Reconstruction for On-The-Go 3D Telepresence</h4>
					<h5><strong>Fuwen Tan</strong>, Chi-Wing Fu, Teng Deng, Jianfei Cai, Tat Jen Cham</h5>
					<h5>ACM Multimedia (ACM MM), 2017, Full paper</h5>
					<p>This paper presents FaceCollage, a robust and real-time system for head reconstruction that can be used to create easy-to-deploy telepresence systems, using a pair of consumer-grade RGBD cameras that provide a wide range of views of the reconstructed user. A key feature is that the system is very simple to rapidly deploy, with autonomous calibration and requiring minimal intervention from the user, other than casually placing the cameras.
					<a href="docs/facecollage_preprint.pdf">...</a>
					</p>
					<h5>[ <a href="docs/facecollage_preprint.pdf">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="videos/facecollage.m4v">video</a>] &nbsp&nbsp&nbsp&nbsp[ <a href="docs/frp087-poster.pptx">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2017_facecollage.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/kinfilter.png">
				</div>
				<div class="col-md-9">
					<h4>High-Quality Kinect Depth Filtering For Real-time 3D Telepresence</h4>
					<h5>Mengyao Zhao, <strong>Fuwen Tan</strong>, Chi-Wing Fu, Chi-Keung Tang, Jianfei Cai, Tat Jen Cham</h5>
					<h5>IEEE International Conf. on Multimedia and Expo (ICME), 2013</h5>
					<p>3D telepresence is a next-generation multimedia application, offering remote users an immersive and natural videoÂ­ conferencing environment with real-time 3D graphics. Kinect sensor, a conswner-grade range camera, facilitates the implementation of some recent 3D telepresence systems. However, conventional data filtering methods are insufficient to handle Kinect depth error because such error is quantized rather than just randomly-distributed <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6607501">...</a>
					</p>
					<h5>[ <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6607501">IEEE Xplorer</a> ] &nbsp&nbsp&nbsp&nbsp[<a href="docs/zhao2013.bib">bibtex</a>]</h5>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/telereg.png">
				</div>
				<div class="col-md-9">
					<h4>Field-guided Registration for Feature-conforming Shape Composition</h4>
					<h5>Hui Huang, Minglun Gong, Daniel Cohen-Or, Yaobin Ouyang, <strong>Fuwen Tan</strong>, Hao Zhang</h5>
					<h5>ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 2012</h5>
					<p>We present an automatic shape composition method to fuse two shape parts which may not overlap and possibly contain sharp features, a scenario often encountered when modeling man-made objects. At the core of our method is a novel field-guided approach to automatically align two input parts in a feature-conforming manner. The key to our field-guided shape registration is a natural continuation of one part into the ambient field <a href="http://vcc.siat.ac.cn/console/homepage/info?id=78">...</a>
					</p>
					<h5>[ <a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/">project</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="http://vcc.siat.ac.cn/commons/fileupload/upload.do?method=download&uploadId=807">paper</a>]&nbsp&nbsp&nbsp&nbsp[<a href="docs/huang2012.bib">bibtex</a>] </h5>
				</div>
			</div> <!-- class="row" -->
		</div> <!-- class="container" -->
	</body>
</html>
