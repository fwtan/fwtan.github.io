
<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta http-equiv="X-UA-Compatible" content="IE=edge">
		<meta name="viewport" content="width=device-width, initial-scale=1">

		<link rel="stylesheet" href="css/bootstrap.min.css">
		<link rel="stylesheet" type="text/css" href="css/style.css">
		<link href='http://fonts.googleapis.com/css?family=Lato:100,300,400' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Roboto:400,300,100' rel='stylesheet' type='text/css'>
		<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400' rel='stylesheet' type='text/css'>

		<script type="text/javascript" src="js/bootstrap.min.js"></script>
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-CHFLM9HMW5"></script>
		<script> 
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-CHFLM9HMW5');
		</script>

		<title>Fuwen's Homepage</title>
	</head>
	
	<body>
		<div class="container">
			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h1 class="title-super">Fuwen Tan</h1>
				</div>
			</div> <!-- class="row" -->
			<div class="row row-thin">
				<div class="col-md-6 text-left">
					<h4 class="header-thin"><a href="docs/fwtan-cv.pdf">Résumé</a></h4>
					<!-- <h4 class="header-thin">Samsung AI Center, Cambridge</h4> -->
				</div>
				<div class="col-md-6 text-right flex-col">
					<!-- <h4 class="header-thin">50/60 Station Road, Cambridge, UK</h4> -->
					<h4 class="header-thin">fuwen.tan@gmail.com</h4>
				</div>
			</div> <!-- class="row" -->
			<div class="row row-thin">
				<div class="col-md-12">
					<hr>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>About me</h2>
				</div>
				<div class="col-md-12 text-thin">
					<p>I am a researcher specializing in Efficient Machine Learning, with a focus lightweight model design,
						data-efficient representation learning, and on-device AI. My work aims to make ML more accessible
						and practical for real-world applications.</p>
				</div>
			</div> 

			<!-- <div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Updates</h2>
				</div>
				<div class="col-md-12 text-thin">
					<p>[01/2023] Our paper <a href="https://arxiv.org/abs/2210.02808">Effective Self-supervised Pre-training on Low-compute Networks without Distillation</a> is accepted to <a class="blue_link" href="https://iclr.cc/Conferences/2023">ICLR 2023</a>. Please find our code in <a class="blue_link" href="https://github.com/fwtan/sslight">SSLight</a>.</p>
					<p>[07/2022] Pleased to be recognized as an <a href="https://icml.cc/Conferences/2022/Reviewers">Outstanding Reviewer </a> for <a href="https://icml.cc/Conferences/2022">ICML 2022</a>.</p>
					<p>[07/2022] Our <a class="blue_link" href="https://arxiv.org/abs/2205.03436">EdgeViTs</a> paper is accepted to <a class="blue_link" href="https://eccv2022.ecva.net/">ECCV 2022</a>. Please find our code in <a class="blue_link" href="https://github.com/saic-fi/edgevit">EdgeViTs</a>.</p>
					<p>[07/2021] Our <a class="blue_link" href="https://arxiv.org/abs/2103.12236">RRT</a> paper is accepted to <a class="blue_link" href="http://iccv2021.thecvf.com/">ICCV 2021</a>. Code and pretrained models are released in <a class="blue_link" href="https://github.com/uvavision/RerankingTransformer">RerankingTransformer</a>.</p>
					<p>[06/2021] I start working as a Researcher in the <a href="https://research.samsung.com/aicenter_cambridge">Samsung AI Center, Cambridge (SAIC-Cambridge)</a>.</p>
					<p>[05/2021] I am recognized as an <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewer</a> for <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.</p>
					<p>[04/2021] I successfully defended my PhD Dissertation: <a href="https://search.lib.virginia.edu/sources/uva_library/items/6395w778v"> Learning Local Representations of Images and Text</a>. </p>
				</div>
			</div> class="row" -->


			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Research</h2>
				</div>
			</div> <!-- class="row" -->


			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="http://arxiv.org/abs/2408.13933">MobileQuant: Mobile-friendly Quantization for On-device Language Models</a></h4>
					<span><strong>Fuwen Tan</strong>, Royson Lee, Lukasz Dudziak, Shell Xu Hu, Sourav Bhattacharya, Timothy Hospedales, Georgios Tzimiropoulos, Brais Martinez
					<h5>Conf. on Empirical Methods in Natural Language Processing, EMNLP Findings, 2024</h5>
					<h5>[ <a href="http://arxiv.org/abs/2408.13933">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/saic-fi/MobileQuant">code</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="docs/2024_mobilequant.bib">bibtex</a>]</h5>
				</div>
			</div>


			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/2210.02808">Effective Self-supervised Pre-training on Low-compute Networks without Distillation</a></h4>
					<span><strong>Fuwen Tan</strong>, Fatemeh Saleh, Brais Martinez
					<h5>International Conference on Learning Representations (ICLR), 2023.</h5>
					<h5>[ <a href="https://arxiv.org/abs/2210.02808">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/fwtan/sslight">code</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/sslight_poster.pdf">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/sslight_slides.pdf">slide</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="docs/2022_lowcompute_ssl.bib">bibtex</a>]</h5>
				</div>
			</div>

			
			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/2206.08339">iBoot: Image-bootstrapped Self-Supervised Video Representation Learning</a></h4>
					Fatemeh Saleh, <span><strong>Fuwen Tan</strong>, Adrian Bulat, Georgios Tzimiropoulos, Brais Martinez
					<h5>[ <a href="https://arxiv.org/abs/2206.08339">paper</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="docs/2022_iboot.bib">bibtex</a>]</h5>
				</div>
			</div>


			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/2205.03436">EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers</a></h4>
					Junting Pan, Adrian Bulat, <span><strong>Fuwen Tan</strong>, Xiatian Zhu, Lukasz Dudziak, Hongsheng Li, Georgios Tzimiropoulos, Brais Martinez
					<h5>European Conference on Computer Vision (ECCV), 2022.</h5>
					<h5>[ <a href="https://arxiv.org/abs/2205.03436">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/saic-fi/edgevit">code</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="docs/2022_edgevits.bib">bibtex</a>]</h5>
				</div>
			</div>


			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/2103.12236">Instance-level Image Retrieval using Reranking Transformers</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="https://sites.google.com/view/jiangbo-yuan/">Jiangbo Yuan</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>International Conference on Computer Vision (ICCV), 2021.</h5>
					<h5>[ <a href="https://arxiv.org/abs/2103.12236">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/RerankingTransformer">code</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="docs/2021_retrieval.bib">bibtex</a>]</h5>
				</div>
			</div>

			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/2001.06001">Curriculum Labeling: Self-paced Pseudo-Labeling for Semi-Supervised Learning</a></h4>
					<span><a href="http://www.cs.virginia.edu/~pc9za/">Paola Cascante-Bonilla</a>, <strong>Fuwen Tan</strong>, <a href="https://www.cs.virginia.edu/yanjun/">Yanjun Qi</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>AAAI Conference on Artificial Intelligence (AAAI), 2021.</h5>
					<h5>[ <a href="https://arxiv.org/abs/2001.06001">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/Curriculum-Labeling">code</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2021_self-paced.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/1911.03826">Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="http://www.cs.virginia.edu/~pc9za/">Paola Cascante-Bonilla</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Xiaoxiao.Guo">Xiaoxiao Guo</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-wuhu">Hui Wu</a>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-sfeng">Song Feng</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>Conf. on Neural Information Processing Systems (NeurIPS), 2019</h5>
					<h5>[ <a href="https://arxiv.org/abs/1911.03826">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/DrillDown">code</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/drilldown_poster.pdf">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2019_drilldown.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/1809.01110">Text2Scene: Generating Compositional Scenes from Textual Descriptions</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-sfeng">Song Feng</a>, <a href="http://vicenteordonez.com">Vicente Ordonez</a></span>
					<h5>Conf. on Computer Vision and Pattern Recognition (CVPR), 2019, <em style="color:#a00">(~Oral presentation + Best Paper Finalist)</em></h5>
					<h5>Posts from <a href="https://news.developer.nvidia.com/ai-model-can-generate-images-from-natural-language-descriptions/">NVIDIA Developer News</a>, <a href="https://www.ibm.com/blogs/research/2019/06/text2scene-textual-descriptions/">IBM Research Blog</a> </h5>
					<h5>[ <a href="https://arxiv.org/abs/1809.01110">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://github.com/uvavision/Text2Scene">code</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/text2scene_poster.pdf">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/text2scene_slides.key">slides</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_text2scene.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">
				<div class="col-md-12 text-thin" >
					<h4><a class="blue_link" href="https://arxiv.org/abs/1706.01021">Where and Who? Automatic Semantic-Aware Person Composition</a></h4>
					<span><strong>Fuwen Tan</strong>, Crispin Bernier, Benjamin Cohen, <a href="http://vicenteordonez.com">Vicente Ordonez</a>, <a href="http://connellybarnes.com">Connelly Barnes</a></span>
					<h5>Winter Conf. on Applications of Computer Vision (WACV), 2018</h5>
					<h5>[ <a href="https://arxiv.org/abs/1706.01021">paper</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="docs/composites_supp.pdf">supplemental PDF</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="https://github.com/fwtan/who_where">code</a> ]&nbsp&nbsp&nbsp&nbsp [ <a href="docs/composites_demo.mp4">video</a> ] &nbsp&nbsp&nbsp&nbsp[ <a href="docs/2018_who_where.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">
				<div class="col-md-12 text-thin">
					<h4><a class="blue_link" href="docs/facecollage_preprint.pdf">FaceCollage: A Rapidly Deployable System for Real-time Head Reconstruction for On-The-Go 3D Telepresence</a></h4>
					<span><strong>Fuwen Tan</strong>, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, Teng Deng, <a href="http://www.ntu.edu.sg/home/asjfcai/">Jianfei Cai</a>, <a href="http://www.ntu.edu.sg/home/astjcham/">Tat Jen Cham </a></span>
			 		<h5>ACM Multimedia (ACM MM, full paper), 2017</h5>
					<h5>[ <a href="docs/facecollage_preprint.pdf">paper</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="videos/facecollage.m4v">video</a>] &nbsp&nbsp&nbsp&nbsp[ <a href="docs/frp087-poster.pptx">poster</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="docs/2017_facecollage.bib">bibtex</a> ]</h5>
				</div>
			</div>

			<div class="row row-thin gray">
				<div class="col-md-12 text-thin">
					<h4>High-Quality Kinect Depth Filtering For Real-time 3D Telepresence</h4>
					<span>Mengyao Zhao, <strong>Fuwen Tan</strong>, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, <a href="http://www.cse.ust.hk/~cktang/">Chi-Keung Tang</a>, <a href="http://www.ntu.edu.sg/home/asjfcai/">Jianfei Cai</a>, <a href="http://www.ntu.edu.sg/home/astjcham/">Tat Jen Cham</a> </span>
					<h5>Conf. on Multimedia and Expo (ICME), 2013</h5>
					<h5>[ <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6607501">IEEE Xplorer</a> ] &nbsp&nbsp&nbsp&nbsp[<a href="docs/zhao2013.bib">bibtex</a>]</h5>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin gray">
				<div class="col-md-12 text-thin">
					<h4>Field-guided Registration for Feature-conforming Shape Composition</h4>
					<span><a href="https://vcc.tech/~huihuang">Hui Huang</a>, <a href="http://www.cs.mun.ca/~gong/">Minglun Gong</a>, <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>, Yaobin Ouyang, <strong>Fuwen Tan</strong>, <a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang</a></span>
					<h5>ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 2012</h5>
					<h5>[ <a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/">project</a> ]&nbsp&nbsp&nbsp&nbsp[<a href="http://web.siat.ac.cn/~vcc/publications/2012/fieldguid/field-guided.pdf">paper</a>]&nbsp&nbsp&nbsp&nbsp[<a href="docs/huang2012.bib">bibtex</a>] </h5>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin">
				<div class="col-md-12 text-left">
					<h2>Thesis</h2>
				</div>
			</div> <!-- class="row" -->

			<div class="row row-thin gray">

				<div class="col-md-3">
					<img class="img-responsive img-embeded" src="images/uva.png", height="500">
				</div>
				<div class="col-md-9 text-thin" >
					<h4><a class="blue_link" href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">PhD Dissertation: Learning Local Representations of Images and Text</a></h4>
					<p style="font-size:90%;">Images and text inherently exhibit hierarchical structures, e.g. scenes built from objects, sentences built from words. In many computer vision and natural language processing tasks, learning accurate prediction models requires analyzing the correlation of the local primitives of both the input and output data. In this thesis, we develop techniques for learning local representations of images and text and demonstrate their effectiveness on visual recognition, retrieval, and synthesis.
					<a href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">...</a>
					</p>
					<h5>[ <a href="https://fwtan.github.io/docs/Fuwen_phd_thesis.pdf">thesis</a> ]&nbsp&nbsp&nbsp&nbsp[ <a href="https://docs.google.com/presentation/d/1MF0lLGM4w9QkDzCGNAUPdZo3cl-AX8vdr8QkHooIUOo/edit?usp=sharing">slides</a> ]</h5>
				</div>
			</div>

		</div> <!-- class="container" -->
	</body>
</html>